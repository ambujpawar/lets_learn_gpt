Holy grail: The paper that started it all
https://arxiv.org/abs/1706.03762

Blog reading list
1. To get an understanding of attention: https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
2. To get an understanding of transformers: https://jalammar.github.io/illustrated-transformer/
3. For a step by step guide to paper by code: http://nlp.seas.harvard.edu/annotated-transformer/
4. 